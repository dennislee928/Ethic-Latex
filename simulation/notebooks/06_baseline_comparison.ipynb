{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline Function Comparison\n",
        "\n",
        "This notebook systematically compares different baseline functions B(x) to determine which best fits the ethical prime distribution Π(x).\n",
        "\n",
        "## Theory\n",
        "\n",
        "The baseline function B(x) represents the expected number of ethical primes up to complexity x. Different forms have different theoretical justifications:\n",
        "\n",
        "1. **Linear**: B(x) = αx - Simple linear growth\n",
        "2. **Prime Theorem**: B(x) = βx/log(x) - Direct analogy to Prime Number Theorem\n",
        "3. **Logarithmic Integral**: B(x) = β·Li(x) where Li(x) = ∫₂ˣ dt/log(t) - Standard form in number theory\n",
        "4. **Power Law**: B(x) = γx^δ - Flexible power law form\n",
        "\n",
        "We compare these using statistical metrics:\n",
        "- **R²**: Coefficient of determination (higher is better)\n",
        "- **AIC**: Akaike Information Criterion (lower is better)\n",
        "- **BIC**: Bayesian Information Criterion (lower is better)\n",
        "- **RMSE**: Root Mean Squared Error (lower is better)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Robust path setup\n",
        "def setup_paths():\n",
        "    current_dir = Path(os.getcwd())\n",
        "    if current_dir.name == 'notebooks':\n",
        "        simulation_dir = str(current_dir.parent)\n",
        "        if simulation_dir not in sys.path:\n",
        "            sys.path.insert(0, simulation_dir)\n",
        "        return simulation_dir\n",
        "    elif current_dir.name == 'simulation':\n",
        "        simulation_dir = str(current_dir)\n",
        "        if simulation_dir not in sys.path:\n",
        "            sys.path.insert(0, simulation_dir)\n",
        "        return simulation_dir\n",
        "    for parent in current_dir.parents:\n",
        "        if parent.name == 'simulation':\n",
        "            simulation_dir = str(parent)\n",
        "            if simulation_dir not in sys.path:\n",
        "                sys.path.insert(0, simulation_dir)\n",
        "            return simulation_dir\n",
        "    for path in ['..', '../simulation', 'simulation']:\n",
        "        abs_path = os.path.abspath(path)\n",
        "        if abs_path not in sys.path:\n",
        "            sys.path.insert(0, abs_path)\n",
        "    return None\n",
        "\n",
        "setup_paths()\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from core.action_space import generate_world\n",
        "from core.judgement_system import BiasedJudge, NoisyJudge, ConservativeJudge, RadicalJudge, evaluate_judgement\n",
        "from core.ethical_primes import select_ethical_primes, compute_Pi_and_error\n",
        "from analysis.baseline_comparison import (\n",
        "    compare_all_baselines, select_best_baseline, generate_baseline_comparison_report\n",
        ")\n",
        "from visualization.plots import setup_paper_style\n",
        "\n",
        "setup_paper_style()\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Data\n",
        "\n",
        "Create a moral action space and evaluate with different judgment systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate action space\n",
        "num_actions = 2000\n",
        "actions = generate_world(num_actions=num_actions, complexity_dist='zipf', random_seed=42)\n",
        "\n",
        "# Create judges to compare\n",
        "judges = {\n",
        "    'Biased': BiasedJudge(bias_strength=0.2, noise_scale=0.1),\n",
        "    'Noisy': NoisyJudge(noise_scale=0.3),\n",
        "    'Conservative': ConservativeJudge(threshold=0.5),\n",
        "    'Radical': RadicalJudge(amplification=1.5)\n",
        "}\n",
        "\n",
        "# Evaluate all judges\n",
        "tau = 0.3\n",
        "results = {}\n",
        "for name, judge in judges.items():\n",
        "    actions_copy = [a for a in actions]  # Copy to avoid modifying original\n",
        "    evaluate_judgement(actions_copy, judge, tau=tau)\n",
        "    primes = select_ethical_primes(actions_copy, importance_quantile=0.9)\n",
        "    results[name] = {\n",
        "        'actions': actions_copy,\n",
        "        'primes': primes\n",
        "    }\n",
        "    print(f\"{name:15s}: {len(primes)} ethical primes\")\n",
        "\n",
        "print(f\"\\nTotal actions: {num_actions}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare Baselines for Each Judge\n",
        "\n",
        "Compare all baseline functions and identify the best fit for each judgment system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_max = 100\n",
        "all_comparisons = {}\n",
        "all_best = {}\n",
        "\n",
        "for judge_name, data in results.items():\n",
        "    primes = data['primes']\n",
        "    \n",
        "    # Compute Pi(x)\n",
        "    Pi_x, _, _, x_vals = compute_Pi_and_error(primes, X_max=X_max, baseline='linear')\n",
        "    \n",
        "    # Compare all baselines\n",
        "    comparison = compare_all_baselines(x_vals, Pi_x, optimize_params=True)\n",
        "    all_comparisons[judge_name] = comparison\n",
        "    \n",
        "    # Select best by AIC\n",
        "    best_type, best_B_x, best_params = select_best_baseline(comparison, criterion='aic')\n",
        "    all_best[judge_name] = {\n",
        "        'type': best_type,\n",
        "        'B_x': best_B_x,\n",
        "        'params': best_params\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{judge_name}:\")\n",
        "    print(f\"  Best baseline: {best_type}\")\n",
        "    print(f\"  R² = {best_params['r_squared']:.4f}\")\n",
        "    print(f\"  AIC = {best_params['aic']:.2f}\")\n",
        "    print(f\"  RMSE = {best_params['rmse']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Comparison Report\n",
        "\n",
        "Generate detailed statistical comparison for each judge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate reports for each judge\n",
        "for judge_name, comparison in all_comparisons.items():\n",
        "    Pi_x, _, _, x_vals = compute_Pi_and_error(\n",
        "        results[judge_name]['primes'], X_max=X_max, baseline='linear'\n",
        "    )\n",
        "    report = generate_baseline_comparison_report(comparison, x_vals, Pi_x)\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"REPORT FOR {judge_name.upper()}\")\n",
        "    print('='*80)\n",
        "    print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization: All Baselines Compared\n",
        "\n",
        "Plot all baseline functions against the actual Π(x) for visual comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison plots for each judge\n",
        "n_judges = len(results)\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "baseline_colors = {\n",
        "    'linear': 'blue',\n",
        "    'prime_theorem': 'red',\n",
        "    'logarithmic_integral': 'green',\n",
        "    'power_law': 'orange'\n",
        "}\n",
        "\n",
        "for idx, (judge_name, comparison) in enumerate(all_comparisons.items()):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Get Pi(x)\n",
        "    Pi_x, _, _, x_vals = compute_Pi_and_error(\n",
        "        results[judge_name]['primes'], X_max=X_max, baseline='linear'\n",
        "    )\n",
        "    \n",
        "    # Plot actual Pi(x)\n",
        "    ax.plot(x_vals, Pi_x, 'ko-', label='Π(x) (actual)', linewidth=2, markersize=4)\n",
        "    \n",
        "    # Plot all baselines\n",
        "    for baseline_type, (B_x, params) in comparison.items():\n",
        "        color = baseline_colors.get(baseline_type, 'gray')\n",
        "        linestyle = '--' if baseline_type == all_best[judge_name]['type'] else '-'\n",
        "        linewidth = 3 if baseline_type == all_best[judge_name]['type'] else 1.5\n",
        "        label = f\"{baseline_type} (R²={params['r_squared']:.3f})\"\n",
        "        ax.plot(x_vals, B_x, color=color, linestyle=linestyle, \n",
        "               linewidth=linewidth, label=label, alpha=0.7)\n",
        "    \n",
        "    ax.set_xlabel('Complexity x', fontsize=12)\n",
        "    ax.set_ylabel('Count', fontsize=12)\n",
        "    ax.set_title(f'{judge_name} Judge', fontsize=14, fontweight='bold')\n",
        "    ax.legend(fontsize=9, loc='best')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../output/figures/06_baseline_comparison.pdf', dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Table\n",
        "\n",
        "Create a summary table comparing baseline performance across all judges.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary DataFrame\n",
        "summary_data = []\n",
        "for judge_name, comparison in all_comparisons.items():\n",
        "    for baseline_type, (B_x, params) in comparison.items():\n",
        "        summary_data.append({\n",
        "            'Judge': judge_name,\n",
        "            'Baseline': baseline_type,\n",
        "            'R²': params['r_squared'],\n",
        "            'AIC': params['aic'],\n",
        "            'BIC': params['bic'],\n",
        "            'RMSE': params['rmse'],\n",
        "            'Is Best': baseline_type == all_best[judge_name]['type']\n",
        "        })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "# Display summary\n",
        "print(\"Summary of Baseline Comparisons:\")\n",
        "print(\"=\" * 80)\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "# Save to CSV\n",
        "os.makedirs('../output', exist_ok=True)\n",
        "summary_df.to_csv('../output/baseline_comparison_summary.csv', index=False)\n",
        "print(f\"\\nSaved summary to: ../output/baseline_comparison_summary.csv\")\n",
        "\n",
        "# Best baseline by judge\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Best Baseline for Each Judge (by AIC):\")\n",
        "print(\"=\" * 80)\n",
        "for judge_name, best_info in all_best.items():\n",
        "    print(f\"{judge_name:15s}: {best_info['type']:20s} \"\n",
        "          f\"(R²={best_info['params']['r_squared']:.4f}, \"\n",
        "          f\"AIC={best_info['params']['aic']:.2f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Term Comparison\n",
        "\n",
        "Compare the error terms E(x) = Π(x) - B(x) for different baselines to see which produces the most \"well-behaved\" error.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot error terms for best baseline of each judge\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, judge_name in enumerate(results.keys()):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Compute error for best baseline\n",
        "    best_type = all_best[judge_name]['type']\n",
        "    Pi_x, B_x, E_x, x_vals = compute_Pi_and_error(\n",
        "        results[judge_name]['primes'], \n",
        "        X_max=X_max, \n",
        "        baseline=best_type\n",
        "    )\n",
        "    \n",
        "    # Plot error term\n",
        "    ax.plot(x_vals, E_x, 'b-', linewidth=2, label=f'E(x) = Π(x) - B(x) [{best_type}]')\n",
        "    ax.axhline(y=0, color='r', linestyle='--', linewidth=1, alpha=0.5)\n",
        "    ax.fill_between(x_vals, -np.abs(E_x), np.abs(E_x), alpha=0.2, color='blue')\n",
        "    \n",
        "    # Add ERH bound visualization (C * x^0.5)\n",
        "    C = np.max(np.abs(E_x)) / np.sqrt(x_vals[-1])\n",
        "    erh_bound = C * np.sqrt(x_vals)\n",
        "    ax.plot(x_vals, erh_bound, 'g--', linewidth=1.5, label='ERH bound: C·√x', alpha=0.7)\n",
        "    ax.plot(x_vals, -erh_bound, 'g--', linewidth=1.5, alpha=0.7)\n",
        "    \n",
        "    ax.set_xlabel('Complexity x', fontsize=12)\n",
        "    ax.set_ylabel('Error E(x)', fontsize=12)\n",
        "    ax.set_title(f'{judge_name} Judge - Error Term', fontsize=14, fontweight='bold')\n",
        "    ax.legend(fontsize=10)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../output/figures/06_error_terms_comparison.pdf', dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions\n",
        "\n",
        "Based on the comparison, we can determine:\n",
        "1. Which baseline form best fits each judge type\n",
        "2. Whether the choice of baseline affects ERH satisfaction\n",
        "3. Recommendations for baseline selection in future analyses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"CONCLUSIONS AND RECOMMENDATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Count how many times each baseline is best\n",
        "baseline_counts = {}\n",
        "for judge_name, best_info in all_best.items():\n",
        "    best_type = best_info['type']\n",
        "    baseline_counts[best_type] = baseline_counts.get(best_type, 0) + 1\n",
        "\n",
        "print(\"\\nBest Baseline Frequency:\")\n",
        "for baseline_type, count in sorted(baseline_counts.items(), key=lambda x: -x[1]):\n",
        "    print(f\"  {baseline_type:25s}: {count}/{len(results)} judges\")\n",
        "\n",
        "print(\"\\nRecommendations:\")\n",
        "print(\"  1. Use 'auto' baseline option to automatically select best fit\")\n",
        "print(\"  2. For theoretical consistency, 'prime_theorem' or 'logarithmic_integral' are preferred\")\n",
        "print(\"  3. Check R² and AIC to ensure good fit quality\")\n",
        "print(\"  4. Consider judge-specific baseline selection for most accurate analysis\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
