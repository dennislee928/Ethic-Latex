{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Judge Comparison Experiment\n",
    "\n",
    "This notebook compares multiple judgment systems to understand which exhibit \"Riemann-healthy\" error behavior.\n",
    "\n",
    "We will compare:\n",
    "- BiasedJudge: Systematic bias increasing with complexity\n",
    "- NoisyJudge: High random noise\n",
    "- ConservativeJudge: Tendency toward neutral judgments  \n",
    "- RadicalJudge: Amplifies extremes\n",
    "\n",
    "For each judge, we compute Π(x), E(x), and test the ERH.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased: ERH=False, α=0.04865433745253869\n",
      "Noisy: ERH=False, α=-0.7820414801504186\n",
      "Conservative: ERH=False, α=-0.13593343557592236\n",
      "Radical: ERH=False, α=-0.15191228480647065\n",
      "# Ethical Riemann Hypothesis - Judge Comparison Report\n",
      "\n",
      "**Number of judges analyzed:** 4\n",
      "\n",
      "## Summary Table\n",
      "\n",
      "| Judge | Actions | Primes | Mistake Rate | MAE | Exponent | ERH Satisfied | Growth Rate |\n",
      "|-------|---------|--------|--------------|-----|----------|---------------|-------------|\n",
      "| Biased | 1000 | 11 | 0.129 | 0.178 | 0.049 | [FAIL] | sublinear_slow |\n",
      "| Noisy | 1000 | 22 | 0.252 | 0.210 | -0.782 | [FAIL] | sublinear_slow |\n",
      "| Conservative | 1000 | 55 | 0.607 | 0.341 | -0.136 | [FAIL] | s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from core.action_space import generate_world\n",
    "from core.judgement_system import BiasedJudge, NoisyJudge, ConservativeJudge, RadicalJudge, batch_evaluate\n",
    "from core.ethical_primes import compare_error_distributions\n",
    "from analysis.statistics import compare_judges, generate_report\n",
    "from visualization.plots import setup_paper_style, plot_multi_judge_errors, plot_judge_comparison\n",
    "\n",
    "setup_paper_style()\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate shared action space\n",
    "actions = generate_world(num_actions=1000, complexity_dist='zipf', random_seed=42)\n",
    "\n",
    "# Create judges\n",
    "judges = {\n",
    "    'Biased': BiasedJudge(bias_strength=0.2, noise_scale=0.1),\n",
    "    'Noisy': NoisyJudge(noise_scale=0.3),\n",
    "    'Conservative': ConservativeJudge(threshold=0.5),\n",
    "    'Radical': RadicalJudge(amplification=1.5)\n",
    "}\n",
    "\n",
    "# Evaluate all\n",
    "results = batch_evaluate(actions, judges, tau=0.3)\n",
    "\n",
    "# Compare\n",
    "comparison = compare_judges(results, X_max=100)\n",
    "for name, metrics in comparison.items():\n",
    "    print(f\"{name}: ERH={metrics.get('erh_satisfied')}, α={metrics.get('estimated_exponent', 'N/A')}\")\n",
    "\n",
    "# Generate report\n",
    "report = generate_report(results, output_path='../output/judge_comparison_report.md')\n",
    "print(report[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
