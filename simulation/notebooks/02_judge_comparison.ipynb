{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Judge Comparison Experiment\n",
    "\n",
    "This notebook compares multiple judgment systems to understand which exhibit \"Riemann-healthy\" error behavior.\n",
    "\n",
    "We will compare:\n",
    "- BiasedJudge: Systematic bias increasing with complexity\n",
    "- NoisyJudge: High random noise\n",
    "- ConservativeJudge: Tendency toward neutral judgments  \n",
    "- RadicalJudge: Amplifies extremes\n",
    "\n",
    "For each judge, we compute Π(x), E(x), and test the ERH.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from core.action_space import generate_world\n",
    "from core.judgement_system import BiasedJudge, NoisyJudge, ConservativeJudge, RadicalJudge, batch_evaluate\n",
    "from core.ethical_primes import compare_error_distributions\n",
    "from analysis.statistics import compare_judges, generate_report\n",
    "from visualization.plots import setup_paper_style, plot_multi_judge_errors, plot_judge_comparison\n",
    "\n",
    "setup_paper_style()\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate shared action space\n",
    "actions = generate_world(num_actions=1000, complexity_dist='zipf', random_seed=42)\n",
    "\n",
    "# Create judges\n",
    "judges = {\n",
    "    'Biased': BiasedJudge(bias_strength=0.2, noise_scale=0.1),\n",
    "    'Noisy': NoisyJudge(noise_scale=0.3),\n",
    "    'Conservative': ConservativeJudge(threshold=0.5),\n",
    "    'Radical': RadicalJudge(amplification=1.5)\n",
    "}\n",
    "\n",
    "# Evaluate all\n",
    "results = batch_evaluate(actions, judges, tau=0.3)\n",
    "\n",
    "# Compare\n",
    "comparison = compare_judges(results, X_max=100)\n",
    "for name, metrics in comparison.items():\n",
    "    print(f\"{name}: ERH={metrics.get('erh_satisfied')}, α={metrics.get('estimated_exponent', 'N/A')}\")\n",
    "\n",
    "# Generate report\n",
    "report = generate_report(results, output_path='../output/judge_comparison_report.md')\n",
    "print(report[:500])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
