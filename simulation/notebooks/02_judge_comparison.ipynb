{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Judge Comparison Experiment\n",
    "\n",
    "This notebook compares multiple judgment systems to understand which exhibit \"Riemann-healthy\" error behavior.\n",
    "\n",
    "We will compare:\n",
    "- BiasedJudge: Systematic bias increasing with complexity\n",
    "- NoisyJudge: High random noise\n",
    "- ConservativeJudge: Tendency toward neutral judgments  \n",
    "- RadicalJudge: Amplifies extremes\n",
    "\n",
    "For each judge, we compute Π(x), E(x), and test the ERH.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased: ERH=False, α=0.04865433745253869\n",
      "Noisy: ERH=False, α=-0.7820414801504186\n",
      "Conservative: ERH=False, α=-0.13593343557592236\n",
      "Radical: ERH=False, α=-0.15191228480647065\n",
      "# Ethical Riemann Hypothesis - Judge Comparison Report\n",
      "\n",
      "**Number of judges analyzed:** 4\n",
      "\n",
      "## Summary Table\n",
      "\n",
      "| Judge | Actions | Primes | Mistake Rate | MAE | Exponent | ERH Satisfied | Growth Rate |\n",
      "|-------|---------|--------|--------------|-----|----------|---------------|-------------|\n",
      "| Biased | 1000 | 11 | 0.129 | 0.178 | 0.049 | [FAIL] | sublinear_slow |\n",
      "| Noisy | 1000 | 22 | 0.252 | 0.210 | -0.782 | [FAIL] | sublinear_slow |\n",
      "| Conservative | 1000 | 55 | 0.607 | 0.341 | -0.136 | [FAIL] | s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Robust path setup\n",
    "def setup_paths():\n",
    "    current_dir = Path(os.getcwd())\n",
    "    if current_dir.name == 'notebooks':\n",
    "        simulation_dir = str(current_dir.parent)\n",
    "        if simulation_dir not in sys.path:\n",
    "            sys.path.insert(0, simulation_dir)\n",
    "        return simulation_dir\n",
    "    elif current_dir.name == 'simulation':\n",
    "        simulation_dir = str(current_dir)\n",
    "        if simulation_dir not in sys.path:\n",
    "            sys.path.insert(0, simulation_dir)\n",
    "        return simulation_dir\n",
    "    for parent in current_dir.parents:\n",
    "        if parent.name == 'simulation':\n",
    "            simulation_dir = str(parent)\n",
    "            if simulation_dir not in sys.path:\n",
    "                sys.path.insert(0, simulation_dir)\n",
    "            return simulation_dir\n",
    "    for path in ['..', '../simulation', 'simulation']:\n",
    "        abs_path = os.path.abspath(path)\n",
    "        if abs_path not in sys.path:\n",
    "            sys.path.insert(0, abs_path)\n",
    "    return None\n",
    "\n",
    "setup_paths()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats as scipy_stats\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WIDGETS_AVAILABLE = False\n",
    "    print(\"Note: ipywidgets not available, interactive features disabled\")\n",
    "\n",
    "from core.action_space import generate_world\n",
    "from core.judgement_system import BiasedJudge, NoisyJudge, ConservativeJudge, RadicalJudge, batch_evaluate, evaluate_judgement\n",
    "from core.ethical_primes import compare_error_distributions\n",
    "from analysis.statistics import compare_judges, generate_report\n",
    "from visualization.plots import setup_paper_style, plot_multi_judge_errors, plot_judge_comparison\n",
    "\n",
    "setup_paper_style()\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set parameters for the comparison experiment. Use interactive widgets if available, otherwise modify values directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "num_actions = 1000\n",
    "tau = 0.3\n",
    "X_max = 100\n",
    "\n",
    "# Judge configurations\n",
    "judge_configs = {\n",
    "    'Biased': {'bias_strength': 0.2, 'noise_scale': 0.1},\n",
    "    'Noisy': {'noise_scale': 0.3},\n",
    "    'Conservative': {'threshold': 0.5},\n",
    "    'Radical': {'amplification': 1.5}\n",
    "}\n",
    "\n",
    "if WIDGETS_AVAILABLE:\n",
    "    # Interactive controls\n",
    "    num_actions_widget = widgets.IntSlider(value=1000, min=500, max=5000, step=500, description='Actions:')\n",
    "    tau_widget = widgets.FloatSlider(value=0.3, min=0.1, max=0.5, step=0.05, description='Tau:')\n",
    "    \n",
    "    def update_experiment(num_actions_val, tau_val):\n",
    "        global num_actions, tau\n",
    "        num_actions = num_actions_val\n",
    "        tau = tau_val\n",
    "        print(f\"Updated: num_actions={num_actions}, tau={tau}\")\n",
    "    \n",
    "    interact = widgets.interactive(update_experiment, num_actions_val=num_actions_widget, tau_val=tau_widget)\n",
    "    display(interact)\n",
    "else:\n",
    "    print(f\"Using fixed parameters: num_actions={num_actions}, tau={tau}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Action Space and Evaluate Judges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate shared action space\n",
    "actions = generate_world(num_actions=num_actions, complexity_dist='zipf', random_seed=42)\n",
    "\n",
    "# Create judges\n",
    "judges = {\n",
    "    'Biased': BiasedJudge(**judge_configs['Biased']),\n",
    "    'Noisy': NoisyJudge(**judge_configs['Noisy']),\n",
    "    'Conservative': ConservativeJudge(**judge_configs['Conservative']),\n",
    "    'Radical': RadicalJudge(**judge_configs['Radical'])\n",
    "}\n",
    "\n",
    "print(f\"Generated {num_actions} actions\")\n",
    "print(f\"Created {len(judges)} judges: {', '.join(judges.keys())}\")\n",
    "\n",
    "# Evaluate all judges\n",
    "results = batch_evaluate(actions, judges, tau=tau)\n",
    "print(\"\\nEvaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Comparison\n",
    "\n",
    "Compare ERH satisfaction and error exponents across judges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare judges\n",
    "comparison = compare_judges(results, X_max=X_max)\n",
    "\n",
    "# Display results\n",
    "print(\"Judge Comparison Results:\")\n",
    "print(\"=\" * 80)\n",
    "for name, metrics in comparison.items():\n",
    "    erh_status = \"SATISFIED\" if metrics.get('erh_satisfied', False) else \"NOT SATISFIED\"\n",
    "    alpha = metrics.get('estimated_exponent', np.nan)\n",
    "    print(f\"{name:15s}: ERH={erh_status:12s}, α={alpha:7.4f}, R²={metrics.get('r_squared', 0):.4f}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(comparison).T\n",
    "comparison_df = comparison_df[['estimated_exponent', 'erh_satisfied', 'r_squared', 'growth_rate']]\n",
    "comparison_df.columns = ['Exponent α', 'ERH Satisfied', 'R²', 'Growth Rate']\n",
    "print(\"\\nComparison Table:\")\n",
    "print(comparison_df.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "### Side-by-Side Error Growth Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot multi-judge error comparison\n",
    "comparison_data = compare_error_distributions(results, X_max=X_max)\n",
    "plot_multi_judge_errors(comparison_data, save_path='../output/figures/02_multi_judge_errors.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Judge Performance Heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of judge performance metrics\n",
    "metrics_data = []\n",
    "for name, metrics in comparison.items():\n",
    "    metrics_data.append({\n",
    "        'Judge': name,\n",
    "        'Exponent α': metrics.get('estimated_exponent', np.nan),\n",
    "        'ERH Satisfied': 1 if metrics.get('erh_satisfied', False) else 0,\n",
    "        'R²': metrics.get('r_squared', 0),\n",
    "        'Mistake Rate': metrics.get('mistake_rate', np.nan)\n",
    "    })\n",
    "\n",
    "heatmap_df = pd.DataFrame(metrics_data).set_index('Judge')\n",
    "heatmap_df_norm = heatmap_df.copy()\n",
    "# Normalize for visualization (except ERH Satisfied which is binary)\n",
    "for col in ['Exponent α', 'R²', 'Mistake Rate']:\n",
    "    if col in heatmap_df_norm.columns:\n",
    "        col_min = heatmap_df_norm[col].min()\n",
    "        col_max = heatmap_df_norm[col].max()\n",
    "        if col_max > col_min:\n",
    "            heatmap_df_norm[col] = (heatmap_df_norm[col] - col_min) / (col_max - col_min)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_df_norm.T, annot=heatmap_df.T, fmt='.3f', cmap='RdYlGn', \n",
    "            cbar_kws={'label': 'Normalized Value'}, linewidths=0.5)\n",
    "plt.title('Judge Performance Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/figures/02_judge_heatmap.pdf', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis\n",
    "\n",
    "### Confidence Intervals and Hypothesis Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis: Bootstrap confidence intervals for exponents\n",
    "def bootstrap_exponent(actions, judge, tau, n_bootstrap=100):\n",
    "    \"\"\"Bootstrap estimate of exponent with confidence interval\"\"\"\n",
    "    exponents = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Resample actions with replacement\n",
    "        indices = np.random.choice(len(actions), size=len(actions), replace=True)\n",
    "        resampled_actions = [actions[i] for i in indices]\n",
    "        \n",
    "        # Evaluate and compute exponent\n",
    "        from core.ethical_primes import select_ethical_primes, compute_Pi_and_error, analyze_error_growth\n",
    "        from core.judgement_system import evaluate_judgement\n",
    "        evaluate_judgement(resampled_actions, judge, tau=tau)\n",
    "        primes = select_ethical_primes(resampled_actions)\n",
    "        if len(primes) > 10:\n",
    "            Pi_x, B_x, E_x, x_vals = compute_Pi_and_error(primes, X_max=X_max)\n",
    "            analysis = analyze_error_growth(E_x, x_vals)\n",
    "            exp = analysis.get('estimated_exponent', np.nan)\n",
    "            if not np.isnan(exp):\n",
    "                exponents.append(exp)\n",
    "    \n",
    "    if len(exponents) > 0:\n",
    "        return {\n",
    "            'mean': np.mean(exponents),\n",
    "            'std': np.std(exponents),\n",
    "            'ci_lower': np.percentile(exponents, 2.5),\n",
    "            'ci_upper': np.percentile(exponents, 97.5)\n",
    "        }\n",
    "    return None\n",
    "\n",
    "print(\"Computing bootstrap confidence intervals (this may take a moment)...\")\n",
    "bootstrap_results = {}\n",
    "for name, judge in judges.items():\n",
    "    result = bootstrap_exponent(actions, judge, tau, n_bootstrap=50)  # Reduced for speed\n",
    "    if result:\n",
    "        bootstrap_results[name] = result\n",
    "        print(f\"{name:15s}: α = {result['mean']:.4f} ± {result['std']:.4f} \"\n",
    "              f\"[{result['ci_lower']:.4f}, {result['ci_upper']:.4f}]\")\n",
    "\n",
    "# Hypothesis test: Is exponent significantly different from 0.5?\n",
    "print(\"\\nHypothesis Tests (H0: α = 0.5):\")\n",
    "print(\"=\" * 80)\n",
    "for name, result in bootstrap_results.items():\n",
    "    # One-sample t-test\n",
    "    t_stat = (result['mean'] - 0.5) / (result['std'] / np.sqrt(50))\n",
    "    p_value = 2 * (1 - scipy_stats.t.cdf(abs(t_stat), df=49))\n",
    "    significant = p_value < 0.05\n",
    "    status = \"REJECT H0\" if significant else \"FAIL TO REJECT H0\"\n",
    "    print(f\"{name:15s}: t={t_stat:7.3f}, p={p_value:.4f}, {status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all metrics for correlation analysis\n",
    "all_metrics = []\n",
    "for name, metrics in comparison.items():\n",
    "    all_metrics.append({\n",
    "        'Judge': name,\n",
    "        'Exponent': metrics.get('estimated_exponent', np.nan),\n",
    "        'R²': metrics.get('r_squared', 0),\n",
    "        'Mistake_Rate': metrics.get('mistake_rate', np.nan),\n",
    "        'MAE': metrics.get('mae', np.nan),\n",
    "        'RMSE': metrics.get('rmse', np.nan)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(all_metrics).set_index('Judge')\n",
    "numeric_cols = metrics_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = metrics_df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Correlation Matrix of Judge Metrics', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/figures/02_correlation_matrix.pdf', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Correlations:\")\n",
    "print(f\"Exponent vs R²: {corr_matrix.loc['Exponent', 'R²']:.3f}\")\n",
    "print(f\"Exponent vs Mistake_Rate: {corr_matrix.loc['Exponent', 'Mistake_Rate']:.3f}\")\n",
    "print(f\"R² vs MAE: {corr_matrix.loc['R²', 'MAE']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Export comparison data and generate detailed report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "os.makedirs('../output', exist_ok=True)\n",
    "\n",
    "# Export comparison table as CSV\n",
    "comparison_df_full = pd.DataFrame(comparison).T\n",
    "csv_path = '../output/judge_comparison_table.csv'\n",
    "comparison_df_full.to_csv(csv_path)\n",
    "print(f\"Exported comparison table to: {csv_path}\")\n",
    "\n",
    "# Export metrics DataFrame\n",
    "metrics_df.to_csv('../output/judge_metrics.csv')\n",
    "print(f\"Exported metrics to: ../output/judge_metrics.csv\")\n",
    "\n",
    "# Generate detailed markdown report\n",
    "report = generate_report(results, output_path='../output/judge_comparison_report.md')\n",
    "print(f\"\\nGenerated detailed report: ../output/judge_comparison_report.md\")\n",
    "print(f\"Report length: {len(report)} characters\")\n",
    "\n",
    "# Display first 1000 characters of report\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Report Preview:\")\n",
    "print(\"=\" * 80)\n",
    "print(report[:1000])\n",
    "print(\"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
